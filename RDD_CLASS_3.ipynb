{"cells":[{"cell_type":"code","source":["sc=spark.sparkContext\nrdd1=sc.parallelize([1,2,3,4,5,6,7],6)\nrdd1.getNumPartitions()\nrdd1.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da878288-7c8b-44df-9a65-085a76f22344","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[2]: ParallelCollectionRDD[1] at readRDDFromInputStream at PythonRDD.scala:435"]}],"execution_count":0},{"cell_type":"code","source":["#glom()\nsc.parallelize([1,2,3,4,5,6,7],6).glom().collect()\n\n#repartition --> increasse or decrease the number of paertitions\n#coalesce -- > reducing the partition \nsc.parallelize([1,2,3,4,5,6,7],6).coalesce(2).glom().collect()\n\n\nsc.parallelize([('a',1),('b',2),('c',3),('a',4)]).countByKey().items()\n\nsc.parallelize([1,1,1,2,3,2]).countByValue().items()\n\na=sc.parallelize([(1,1),(1,2),(3,2)]).collectAsMap()\nprint(a)\na[3]\n\nsc.parallelize([1,2,3,4,5,6,7],6).take(2)\nsc.parallelize([1,2,3,4,5,6,7],6).count()\n#driver memory out of space --- large dataset \nsc.parallelize([1,2,3,4,5,6,7]).flatMap(lambda x : range(1,x)).collect()\n\na=sc.parallelize([1,2,3,4,5,6,7]).groupBy(lambda x : x%2).collect()\nsorted([(x , sorted(y)) for (x,y) in a])\n\n\nsc.parallelize([('a',1),('b',2),('c',3),('a',4)]).groupByKey().mapValues(list).collect()\n\n\n\na=sc.parallelize([1,2,3,4,])\nb=sc.parallelize([1,2,3,4,5,6,7])\n\na.union(b).collect()\na.intersection(b).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1fae8860-8acf-4f33-a2a3-f15519667747","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["{1: 2, 3: 2}\nOut[22]: [1, 2, 3, 4]"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Jan-RDD-class-3","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
